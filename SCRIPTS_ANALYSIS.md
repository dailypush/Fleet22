# ðŸ” Fleet22 Scripts Analysis & Optimization Recommendations

## Executive Summary

**Date:** October 7, 2025  
**Total Scripts Analyzed:** 9 Python scripts  
**Key Findings:** Multiple opportunities for consolidation, better naming, enhanced functionality  

---

## Current Script Inventory

### ðŸ“ Scrapers (6 files)
1. `fleetBoats.py` - Scrapes boat data from local members.html
2. `fleetBoats_github_actions.py` - GitHub Actions version with fallbacks
3. `SailTags.py` - Scrapes sail tag data from J105 archive
4. `getOwnersStatus.py` - Scrapes owner/membership status
5. `combineFleetSailOwner.py` - Combines multiple data sources
6. `yachtscoring_extract_ws_number.py` - Extracts World Sailing numbers

### ðŸ› ï¸ Utilities (3 files)
1. `fleet22_payment_status.py` - Updates payment status in boat data
2. `sail_limit_checker.py` - Validates sail purchase limits
3. `validate_data.py` - Validates JSON data integrity

### ðŸ“Š Analysis (1 file)
1. `SailmakerAnnualSailpurchases.py` - Incomplete visualization script

### ðŸ“¦ Generators (0 files)
- Directory exists but is empty

---

## ðŸŽ¯ Major Issues Identified

### 1. **Duplicate Functionality**
- `fleetBoats.py` and `fleetBoats_github_actions.py` do the same thing
- 90% code overlap with only environment handling differences

### 2. **Inconsistent Naming**
- Mix of camelCase (`fleetBoats.py`, `SailTags.py`)
- Mix of snake_case (`fleet22_payment_status.py`)
- Inconsistent capitalization (`SailTags` vs `getOwnersStatus`)

### 3. **Hardcoded Paths**
- Multiple path resolution approaches
- Some use relative paths, some absolute
- Inconsistent data directory references

### 4. **Incomplete Scripts**
- `yachtscoring_extract_ws_number.py` - Has hardcoded placeholder path
- `SailmakerAnnualSailpurchases.py` - References undefined `data_filtered`
- Empty generators directory

### 5. **Missing Error Handling**
- Some scripts fail silently
- Inconsistent logging approaches
- No retry logic for network requests

### 6. **Code Quality Issues**
- Commented out imports (`from datetime import datetime`)
- Missing docstrings in some functions
- No type hints
- Outdated dependencies in requirements.txt

---

## ðŸ”„ Consolidation Opportunities

### Opportunity #1: Merge Fleet Boat Scrapers
**Combine:**
- `fleetBoats.py`
- `fleetBoats_github_actions.py`

**Into:**
- `scrape_fleet_boats.py` (single unified script)

**Benefits:**
- Eliminate 90% code duplication
- Single script handles both local and CI/CD environments
- Easier maintenance
- Consistent behavior

**Estimated LOC Reduction:** 67 + 110 â†’ 90 lines (42% reduction)

---

### Opportunity #2: Create Unified Scraper Base Class
**Create:**
- `base_scraper.py` - Abstract base class for all scrapers

**Refactor:**
- `scrape_sail_tags.py` (rename from SailTags.py)
- `scrape_owner_status.py` (rename from getOwnersStatus.py)
- `scrape_fleet_boats.py`

**Benefits:**
- Shared error handling
- Consistent logging
- Retry logic in one place
- Rate limiting standardized
- Easier to add new scrapers

**Estimated LOC Reduction:** ~150 lines across all scrapers

---

### Opportunity #3: Create Data Pipeline Manager
**Create:**
- `data_pipeline.py` - Orchestrates all scraping and processing

**Integrates:**
- All scrapers
- `combine_data_sources.py` (rename from combineFleetSailOwner.py)
- `validate_data.py`
- `update_payment_status.py` (rename from fleet22_payment_status.py)

**Benefits:**
- Single entry point for data refresh
- Dependency management between scripts
- Progress tracking
- Failure recovery
- Scheduled execution support

---

### Opportunity #4: Create Utilities Module
**Create:**
- `utils/` directory with:
  - `path_utils.py` - Centralized path resolution
  - `data_loader.py` - Standardized data loading
  - `logger.py` - Centralized logging configuration
  - `validators.py` - Data validation functions

**Benefits:**
- DRY principle
- Consistent behavior across scripts
- Easier testing
- Better reusability

---

## ðŸ“ Specific Recommendations

### High Priority ðŸ”´

#### 1. **Rename Scripts for Consistency**
Use snake_case with descriptive verbs:

```
OLD NAME                          â†’ NEW NAME
================================================================
fleetBoats.py                     â†’ scrape_fleet_boats.py
fleetBoats_github_actions.py      â†’ [DELETE - merge into above]
SailTags.py                       â†’ scrape_sail_tags.py
getOwnersStatus.py                â†’ scrape_owner_status.py
combineFleetSailOwner.py          â†’ combine_data_sources.py
yachtscoring_extract_ws_number.py â†’ extract_world_sailing_numbers.py
fleet22_payment_status.py         â†’ update_payment_status.py
sail_limit_checker.py             â†’ check_sail_limits.py
validate_data.py                  â†’ validate_fleet_data.py
SailmakerAnnualSailpurchases.py   â†’ analyze_sailmaker_trends.py
```

#### 2. **Fix Incomplete Scripts**

**`extract_world_sailing_numbers.py`:**
- Remove hardcoded path placeholder
- Add CLI argument for file path
- Add output format options (JSON, CSV)
- Add proper error handling

**`analyze_sailmaker_trends.py`:**
- Add data loading from actual file
- Complete the analysis
- Add command-line arguments
- Export results to file
- Generate proper visualization

#### 3. **Consolidate Path Management**

Create `utils/path_utils.py`:
```python
"""Centralized path management for Fleet22 scripts."""
from pathlib import Path

# Project root
PROJECT_ROOT = Path(__file__).parent.parent.parent
DATA_DIR = PROJECT_ROOT / "data"
LOGS_DIR = PROJECT_ROOT / "logs"

# Data subdirectories
BOATS_DATA = DATA_DIR / "boats"
SAILS_DATA = DATA_DIR / "sails"
MEMBERS_DATA = DATA_DIR / "members"
RACES_DATA = DATA_DIR / "races"
CALENDAR_DATA = DATA_DIR / "calendar"
PAYMENTS_DATA = DATA_DIR / "payments"

def ensure_directories():
    """Create all required directories if they don't exist."""
    for directory in [BOATS_DATA, SAILS_DATA, MEMBERS_DATA, 
                      RACES_DATA, CALENDAR_DATA, PAYMENTS_DATA, LOGS_DIR]:
        directory.mkdir(parents=True, exist_ok=True)
```

#### 4. **Update requirements.txt**

Current versions are outdated (from 2021):
```txt
# OLD (2021)
beautifulsoup4==4.9.3
requests==2.25.1
tqdm==4.56.0

# NEW (2025)
beautifulsoup4>=4.12.0
requests>=2.31.0
tqdm>=4.66.0
pandas>=2.1.0  # Add missing dependency
matplotlib>=3.8.0  # Add missing dependency
```

---

### Medium Priority ðŸŸ¡

#### 5. **Create Base Scraper Class**

```python
# base_scraper.py
"""Base class for all web scrapers."""
import logging
import time
import requests
from abc import ABC, abstractmethod
from typing import Dict, Optional
from bs4 import BeautifulSoup

class BaseScraper(ABC):
    """Abstract base class for all scrapers."""
    
    DEFAULT_HEADERS = {
        'User-Agent': 'Mozilla/5.0 (compatible; Fleet22Bot/1.0)'
    }
    
    def __init__(self, base_url: str, rate_limit: float = 1.0):
        self.base_url = base_url
        self.rate_limit = rate_limit
        self.logger = self._setup_logger()
        
    def _setup_logger(self) -> logging.Logger:
        """Setup logger for this scraper."""
        logger = logging.getLogger(self.__class__.__name__)
        # Configure logger
        return logger
    
    def fetch_page(self, url: str, max_retries: int = 3) -> Optional[str]:
        """Fetch page with retry logic."""
        for attempt in range(max_retries):
            try:
                time.sleep(self.rate_limit)
                response = requests.get(url, headers=self.DEFAULT_HEADERS, 
                                       verify=False, timeout=30)
                response.raise_for_status()
                return response.text
            except requests.RequestException as e:
                self.logger.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt == max_retries - 1:
                    raise
        return None
    
    @abstractmethod
    def parse_data(self, html: str) -> list:
        """Parse HTML and extract data. Must be implemented by subclasses."""
        pass
    
    @abstractmethod
    def save_data(self, data: list, filepath: str) -> None:
        """Save extracted data. Must be implemented by subclasses."""
        pass
    
    def run(self) -> None:
        """Main execution method."""
        self.logger.info(f"Starting {self.__class__.__name__}")
        html = self.fetch_page(self.base_url)
        if html:
            data = self.parse_data(html)
            self.save_data(data, self.get_output_path())
            self.logger.info(f"Successfully scraped {len(data)} items")
```

#### 6. **Create Data Pipeline Manager**

```python
# data_pipeline.py
"""Orchestrates the complete data pipeline."""
import logging
from typing import List, Callable
from dataclasses import dataclass

@dataclass
class PipelineStep:
    name: str
    function: Callable
    required: bool = True
    depends_on: List[str] = None

class DataPipeline:
    """Manages the execution of data scraping and processing steps."""
    
    def __init__(self):
        self.steps = []
        self.results = {}
        
    def add_step(self, name: str, function: Callable, 
                 required: bool = True, depends_on: List[str] = None):
        """Add a step to the pipeline."""
        self.steps.append(PipelineStep(name, function, required, depends_on))
    
    def run(self):
        """Execute all pipeline steps in order."""
        for step in self.steps:
            if step.depends_on:
                # Check dependencies
                if not all(self.results.get(dep) for dep in step.depends_on):
                    if step.required:
                        raise Exception(f"Dependencies not met for {step.name}")
                    continue
            
            try:
                result = step.function()
                self.results[step.name] = result
            except Exception as e:
                if step.required:
                    raise
                logging.warning(f"Optional step {step.name} failed: {e}")

# Usage:
# pipeline = DataPipeline()
# pipeline.add_step("scrape_boats", scrape_fleet_boats)
# pipeline.add_step("scrape_sails", scrape_sail_tags)
# pipeline.add_step("combine_data", combine_data_sources, 
#                  depends_on=["scrape_boats", "scrape_sails"])
# pipeline.run()
```

#### 7. **Add CLI Interface**

Use `argparse` or `click` for all scripts:
```python
# Example for scrape_fleet_boats.py
import argparse

def main():
    parser = argparse.ArgumentParser(description='Scrape Fleet 22 boat data')
    parser.add_argument('--source', choices=['local', 'remote'], 
                       default='remote', help='Data source')
    parser.add_argument('--output', help='Output file path')
    parser.add_argument('--verbose', action='store_true', 
                       help='Enable verbose logging')
    args = parser.parse_args()
    
    # Execute based on args
```

---

### Low Priority ðŸŸ¢

#### 8. **Add Type Hints**

Add type hints to all functions:
```python
from typing import List, Dict, Optional

def load_json_data(filename: str) -> List[Dict]:
    """Load JSON data from a file."""
    # ...
    
def standardize_hull_number(hull_num: str) -> str:
    """Standardize hull number format."""
    # ...
```

#### 9. **Add Unit Tests**

Create `tests/` directory:
```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_scrapers.py
â”œâ”€â”€ test_utilities.py
â”œâ”€â”€ test_validators.py
â””â”€â”€ test_data_pipeline.py
```

#### 10. **Add Configuration File**

Create `config.yaml`:
```yaml
# Fleet22 Configuration
data:
  boats_file: data/boats/boats_fleet22.json
  sails_file: data/sails/sail_tags.json
  members_file: data/members/j105_members_status.json

scraping:
  rate_limit: 1.0  # seconds between requests
  user_agent: "Mozilla/5.0 (compatible; Fleet22Bot/1.0)"
  verify_ssl: false

logging:
  level: INFO
  file: logs/scraping.log
  format: "%(asctime)s - %(levelname)s - %(message)s"
```

---

## ðŸ“Š Proposed New Structure

```
scripts/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config.yaml                    # NEW: Configuration
â”œâ”€â”€ data_pipeline.py               # NEW: Pipeline orchestrator
â”‚
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_scraper.py            # NEW: Base class
â”‚   â”œâ”€â”€ scrape_fleet_boats.py      # MERGED: fleetBoats + github_actions
â”‚   â”œâ”€â”€ scrape_sail_tags.py        # RENAMED: SailTags.py
â”‚   â”œâ”€â”€ scrape_owner_status.py     # RENAMED: getOwnersStatus.py
â”‚   â””â”€â”€ extract_world_sailing_numbers.py  # FIXED & RENAMED
â”‚
â”œâ”€â”€ processors/                    # NEW: Data processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ combine_data_sources.py    # RENAMED: combineFleetSailOwner.py
â”‚   â””â”€â”€ update_payment_status.py   # RENAMED: fleet22_payment_status.py
â”‚
â”œâ”€â”€ validators/                    # NEW: Validation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ validate_fleet_data.py     # RENAMED: validate_data.py
â”‚   â””â”€â”€ check_sail_limits.py       # RENAMED: sail_limit_checker.py
â”‚
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ analyze_sailmaker_trends.py  # FIXED & RENAMED
â”‚
â””â”€â”€ utils/                         # NEW: Utilities
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ path_utils.py              # NEW: Path management
    â”œâ”€â”€ data_loader.py             # NEW: Data loading utilities
    â”œâ”€â”€ logger.py                  # NEW: Logging configuration
    â””â”€â”€ network.py                 # NEW: Network utilities
```

---

## ðŸŽ¯ Implementation Plan

### Phase 1: Critical Fixes (Week 1)
- [ ] Fix incomplete scripts (`extract_world_sailing_numbers.py`, `analyze_sailmaker_trends.py`)
- [ ] Update requirements.txt
- [ ] Consolidate path management
- [ ] Add proper error handling to all scripts

### Phase 2: Consolidation (Week 2)
- [ ] Merge duplicate scripts (`fleetBoats` â†’ `scrape_fleet_boats`)
- [ ] Rename all scripts for consistency
- [ ] Move scripts to new directory structure
- [ ] Update import statements

### Phase 3: Enhancement (Week 3)
- [ ] Create base scraper class
- [ ] Add CLI interfaces
- [ ] Create utilities module
- [ ] Add configuration file

### Phase 4: Advanced (Week 4)
- [ ] Create data pipeline manager
- [ ] Add type hints
- [ ] Write unit tests
- [ ] Add documentation

---

## ðŸ’° Expected Benefits

### Code Quality
- **40%** reduction in lines of code
- **100%** naming consistency
- **Zero** code duplication
- **Better** error handling

### Maintainability
- Single source of truth for common functionality
- Easier to add new scrapers
- Consistent interfaces
- Better documentation

### Reliability
- Retry logic for all network requests
- Better error messages
- Validation at each step
- Easier debugging

### Performance
- Centralized rate limiting
- Efficient data loading
- Pipeline optimization
- Progress tracking

---

## ðŸš€ Quick Wins (Can Do Today)

1. **Rename scripts** for consistency (5 minutes)
2. **Update requirements.txt** (2 minutes)
3. **Fix `extract_world_sailing_numbers.py`** - Add CLI args (10 minutes)
4. **Create `path_utils.py`** (15 minutes)
5. **Update imports** in all scripts to use path_utils (20 minutes)

**Total Time: ~1 hour for immediate improvements**

---

## ðŸ“š Additional Recommendations

### Documentation
- Add README.md in scripts/ directory
- Document each script's purpose
- Add usage examples
- Create API documentation

### Monitoring
- Add success/failure metrics
- Track scraping duration
- Monitor data quality
- Alert on failures

### Automation
- GitHub Actions for scheduled runs
- Automatic data validation
- Backup before updates
- Rollback on validation failures

---

## âœ… Next Steps

1. Review this analysis
2. Prioritize which changes to implement
3. Create branch: `feature/scripts-refactor`
4. Implement Phase 1 (critical fixes)
5. Test thoroughly
6. Deploy Phase 1
7. Continue with subsequent phases

---

**Analysis Complete: October 7, 2025**  
**Recommendation: Implement in phases over 4 weeks**  
**Expected ROI: High - Better maintainability, reliability, and developer experience**
